<html>
    <head>
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.13.0/dist/tf.min.js"></script>
        <script>
            function getSize() {
                const m = document.URL.match(/(\d+)x(\d+)/)
                if (m) {
                    return {
                        width: parseInt(m[1]),
                        height: parseInt(m[2]),
                    }
                }
                return {
                    width: 320,
                    height: 240,
                }
            }

            function getColor() {
                const m = document.URL.match(/(\d+),(\d+),(\d+)/)
                if (m) {
                    return m[0]
                }
                return '0,255,0'
            }

            async function main() {
                tf.setBackend('webgl');
                console.log(tf.getBackend());

                const video = document.querySelector('video');
                const canvas = document.querySelector('canvas');
                canvas.style.background = 'rgb(' + getColor() + ')';

                const { width, height } = getSize()
                video.width = width;
                video.height = height;
                const webcam = await tf.data.webcam(video);
                const model = await tf.loadGraphModel('model/model.json');

                // Set initial recurrent state
                let [r1i, r2i, r3i, r4i] = [tf.tensor(0.), tf.tensor(0.), tf.tensor(0.), tf.tensor(0.)];

                // Set downsample ratio
                const downsample_ratio = tf.tensor(0.5);

                // Inference loop
                async function step() {
                    await tf.nextFrame();
                    const img = await webcam.capture();
                    const src = tf.tidy(() => img.expandDims(0).div(255)); // normalize input
                    const [fgr, pha, r1o, r2o, r3o, r4o] = await model.executeAsync(
                        {src, r1i, r2i, r3i, r4i, downsample_ratio}, // provide inputs
                        ['fgr', 'pha', 'r1o', 'r2o', 'r3o', 'r4o']   // select outputs
                    );

                    await drawMatte(fgr.clone(), pha.clone(), canvas);

                    // Dispose old tensors.
                    tf.dispose([img, src, fgr, pha, r1i, r2i, r3i, r4i]);

                    // Update recurrent states.
                    [r1i, r2i, r3i, r4i] = [r1o, r2o, r3o, r4o];
                    window.requestAnimationFrame(step)
                }
                window.requestAnimationFrame(step)
            }

            async function drawMatte(fgr, pha, canvas){
                const rgba = tf.tidy(() => {
                    const rgb = (fgr !== null) ?
                        fgr.squeeze(0).mul(255).cast('int32') :
                        tf.fill([pha.shape[1], pha.shape[2], 3], 255, 'int32');
                    const a = (pha !== null) ?
                        pha.squeeze(0).mul(255).cast('int32') :
                        tf.fill([fgr.shape[1], fgr.shape[2], 1], 255, 'int32');
                    return tf.concat([rgb, a], -1);
                });
                fgr && fgr.dispose();
                pha && pha.dispose();
                const [height, width] = rgba.shape.slice(0, 2);
                const pixelData = new Uint8ClampedArray(await rgba.data());
                const imageData = new ImageData(pixelData, width, height);
                canvas.width = width;
                canvas.height = height;
                canvas.getContext('2d').putImageData(imageData, 0, 0);
                rgba.dispose();
            }

            window.addEventListener('load', main);
        </script>
        <style>
            video {
                display: none;
            }
            body {
                margin: 0;
            }
        </style>
    </head>
    <body>
        <video></video>
        <canvas></canvas>
    </body>
</html>